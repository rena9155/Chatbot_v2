{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "dneivYwDpv7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gk2J2sYYjTkM"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL FIRST!\n",
        "!pip install -q langchain pypdf2 tiktoken textract openai faiss-cpu huggingface_hub pypdfium2 InstructorEmbedding sentence-transformers googletrans==3.1.0a0 python-docx contractions -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "W7Wvff5rimUR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fKTP5nWE62ra"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "from langchain import LLMChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "import os\n",
        "import contractions\n",
        "import pypdfium2 as pdfium\n",
        "import re\n",
        "import glob\n",
        "import unicodedata\n",
        "from typing import List\n",
        "from googletrans import Translator\n",
        "import time\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download documents to the file section of the notebook"
      ],
      "metadata": {
        "id": "9v79YcWOGiKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_id = '1O4AO1HxRGbL-IfG__UpJORo50Hl0oN9z'"
      ],
      "metadata": {
        "id": "NSzXLyH7Hhv6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "folders = [folder_id]\n",
        "\n",
        "for folder in folders:\n",
        "  file_list = drive.ListFile({'q': f\"'{folder}' in parents and trashed=false\"}).GetList()\n",
        "  for file in file_list:\n",
        "    file_id = file['id']\n",
        "    file_name = file['title']\n",
        "\n",
        "    downloaded = drive.CreateFile({'id': file_id})\n",
        "    downloaded.FetchMetadata(fetch_all=True)\n",
        "    downloaded.GetContentFile(downloaded.metadata['title'])\n"
      ],
      "metadata": {
        "id": "evqOtGJ7Gs2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "czillhUki-Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "j4nqjFT6iY3i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6zJhopOCukeg"
      },
      "outputs": [],
      "source": [
        "#translate answer to English if the question is brought in another language\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "def detect_and_translate(text, answer=None):\n",
        "    result_lang = translator.detect(text)\n",
        "\n",
        "    if result_lang.lang == \"en\":\n",
        "        translate_text = text if answer is None else answer\n",
        "    else:\n",
        "        dest_lang = \"en\" if answer is None else result_lang.lang\n",
        "        translate_text = translator.translate(text, src='auto', dest=dest_lang).text\n",
        "\n",
        "    return translate_text\n",
        "\n",
        "\n",
        "\n",
        "#convert text to document objects\n",
        "def text_to_docs(text: str) -> List[Document]:\n",
        "    if isinstance(text, str):\n",
        "        # Take a single string as one page\n",
        "        text = [text]\n",
        "    page_docs = [Document(page_content=page) for page in text]\n",
        "    # Add page numbers as metadata\n",
        "    for i, doc in enumerate(page_docs):\n",
        "        doc.metadata[\"page\"] = i + 1\n",
        "\n",
        "    return doc\n",
        "\n",
        "#clean text data\n",
        "def text_clean(text: str) -> str:\n",
        "    text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)|(?<!\\r\\s)\\r(?!\\s\\r)\", \" \", text.strip())\n",
        "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "#convert pdf to text\n",
        "def pdfium_get_text(data: bytes) -> str:\n",
        "    output = \"\"\n",
        "    pdf = pdfium.PdfDocument(data)\n",
        "    for i in range(len(pdf)):\n",
        "        page = pdf.get_page(i)\n",
        "        textpage = page.get_textpage()\n",
        "        text = textpage.get_text_range()\n",
        "        text = text_clean(text)\n",
        "        output+=text + \"\\n\"\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing all pdf files"
      ],
      "metadata": {
        "id": "iRMEHCJgpG9S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fdgg9uLa50tp"
      },
      "outputs": [],
      "source": [
        "docs=[]\n",
        "pdf_files =glob.glob('/content/*.pdf')\n",
        "for pdf in pdf_files:\n",
        "  with open(pdf,\"rb\") as f:\n",
        "    data = f.read()\n",
        "    entire_text=pdfium_get_text(data)\n",
        "    mydoc=text_to_docs(entire_text)\n",
        "    docs.append(mydoc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting docs into chunks"
      ],
      "metadata": {
        "id": "bw614_aXpZl1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y4zGdYykl9L8"
      },
      "outputs": [],
      "source": [
        "#split documents\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=80,length_function=len)\n",
        "chunked_docs = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ],
      "metadata": {
        "id": "dBMn-8cc3AKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key_hf = json.load(open(\"hf_credential.json\"))\n",
        "HUGGINGFACEHUB_API_TOKEN=key_hf['key']"
      ],
      "metadata": {
        "id": "4775V4kSHEeP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOJvdKzuyHs_",
        "outputId": "c11ef7f7-c514-4e79-ae3b-85d175314720"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import trange\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create embedding out of content and save it\n",
        "embedding = FAISS.from_documents(chunked_docs,embedding_model)\n",
        "embedding.save_local(\"/content/drive/MyDrive/Chatbot/data/index/\")"
      ],
      "metadata": {
        "id": "cwBTOq3o3EZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load embedding\n",
        "embedding = FAISS.load_local(\"/content/drive/MyDrive/Chatbot/data/index\",embedding_model)"
      ],
      "metadata": {
        "id": "M7DNj30t8M94"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doc_to_rscope(docs):\n",
        "  rscope = \"\"\n",
        "  for doc in docs:\n",
        "    rscope += doc.page_content + \"\\n\\n\"\n",
        "  return rscope"
      ],
      "metadata": {
        "id": "mEeKfMbSZti5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load OPENAI credentials"
      ],
      "metadata": {
        "id": "urAVHe-4E8DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "key_openai = json.load(open(\"openai_credential.json\"))\n",
        "os.environ['OPENAI_API_KEY'] = key_openai['key'][0]"
      ],
      "metadata": {
        "id": "qhHH0aZ2FXsZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA chatbot"
      ],
      "metadata": {
        "id": "TWVmoijIHXLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def qa_chatbot(query):\n",
        "  scope_doc = embedding.similarity_search(query=query)\n",
        "  context = doc_to_rscope(scope_doc)\n",
        "  memory = ConversationBufferMemory(k=10,memory_key='chat_history')\n",
        "  chat_text = \"\"\"\n",
        "  Given the following context information about statistics knowledge\n",
        "\n",
        "  Act as a frequently asked questions chatbot and answer any question\n",
        "  asked factually with the right information.\n",
        "\n",
        "  If the answer cannot be found in the context, answer 'The question asked\n",
        "  is beyond the scope of this textbook'\n",
        "\n",
        "  Context: {context}\n",
        "  Question: {query}\n",
        "  \"\"\"\n",
        "  llm = ChatOpenAI(model='gpt-3.5-turbo',temperature = 0.1)\n",
        "  prompt_template = ChatPromptTemplate.from_template(chat_text)\n",
        "  chatgpt_llm_chain = LLMChain(prompt=prompt_template, llm=llm)\n",
        "  answer = chatgpt_llm_chain.run(context=chat_text,\n",
        "                            query=query)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "eJifIjVUGVmj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chatbot(query = 'what is a random variable?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "5GAyp_O3J3EE",
        "outputId": "0bfbb38f-9347-4af4-ae4a-8e4b9c4ef2ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A random variable is a variable that can take on different values based on the outcome of a random event. It represents a numerical quantity that is determined by chance. Random variables can be discrete, meaning they can only take on specific values, or continuous, meaning they can take on any value within a certain range. Random variables are a fundamental concept in statistics and probability theory.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chatbot(query = 'what is the central limit theorem')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "q7c1ThT1RQFF",
        "outputId": "a145333d-507a-49fe-d796-eca2749831ba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The central limit theorem states that when independent random variables are added, their sum tends toward a normal distribution, regardless of the shape of the original variables' distribution. This theorem is important in statistics because it allows us to make inferences about a population based on a sample.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}